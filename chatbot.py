# /// script
# requires-python = ">=3.8"
# dependencies = [
#     "textual>=0.40.0",
#     "langchain-ollama>=0.1.0",
#     "langchain-core>=0.2.0",
# ]
# ///

from textual import on, work
from textual.app import App, ComposeResult
from textual.widgets import Input, Footer, RichLog, Static
from textual.events import Key, Paste
from rich.console import Console
from rich.panel import Panel
from rich.text import Text
from rich.markdown import Markdown as RichMarkdown
from rich.live import Live
from langchain_ollama import ChatOllama
from langchain_core.messages import HumanMessage, AIMessage, SystemMessage
import requests
import pyperclip

# ChatMessage í´ë˜ìŠ¤ ì œê±° - rich printë¡œ ëŒ€ì²´

class LangChainOllamaChat(App):
    """LangChain Ollamaë¥¼ ì‚¬ìš©í•œ ì±„íŒ… ì•±"""
    
    # í´ë¦½ë³´ë“œ ì•¡ì„¸ìŠ¤ í™œì„±í™”
    ENABLE_COMMAND_PALETTE = False
    
    CSS = """
    #chat-log {
        height: 1fr;
        margin: 0;
        padding: 1;
        scrollbar-size: 0 0;
    }
    
    #streaming-response {
        dock: bottom;
        height: auto;
        margin: 0 1;
        padding: 1;
        border: round yellow;
    }
    
    #streaming-response.hidden {
        display: none;
    }
    
    #chat-input {
        dock: bottom;
        margin: 0;
        border: round $primary;
    }
    """
    
    def __init__(self):
        super().__init__()
        self.ollama_url = "http://localhost:11434"
        self.model_name = "qwen3:30b-32k"  # ê¸°ë³¸ ëª¨ë¸
        self.chat_model = None
        self.messages = []  # LangChain ë©”ì‹œì§€ íˆìŠ¤í† ë¦¬
        self.available_models = []
        self.console = Console()  # Rich ì½˜ì†” ì¸ìŠ¤í„´ìŠ¤
        self.current_ai_widget = None  # í˜„ì¬ AI ì‘ë‹µ ìœ„ì ¯ ì¶”ì 
        
    def compose(self) -> ComposeResult:
        """UI êµ¬ì„± - RichLogì™€ ì…ë ¥ì°½"""
        yield RichLog(id="chat-log", highlight=True, markup=True)
        yield Static(id="streaming-response", classes="hidden")  # ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µìš© ìœ„ì ¯
        yield Input(value="> ", placeholder="ë©”ì‹œì§€ë¥¼ ì…ë ¥í•˜ì„¸ìš”... (ì¢…ë£Œí•˜ë ¤ë©´ 'quit' ì…ë ¥)", id="chat-input")
        # yield Footer()

    def on_mount(self) -> None:
        """ì•± ì‹œì‘ì‹œ ì‹¤í–‰"""
        # ì‹œì‘ ë©”ì‹œì§€ë¥¼ Richë¡œ ì¶œë ¥
        self.print_welcome_message()
        
        if self.check_ollama_connection():
            self.initialize_chat_model()
            self.load_available_models()
        
    def print_welcome_message(self):
        """í™˜ì˜ ë©”ì‹œì§€ë¥¼ RichLogì— ì¶œë ¥"""
        chat_log = self.query_one("#chat-log")
        welcome_panel = Panel(
            RichMarkdown("""# ğŸ¦™ LangChain Ollama ì±„íŒ…ë´‡

ë©”ì‹œì§€ë¥¼ ì…ë ¥í•´ì„œ ëŒ€í™”ë¥¼ ì‹œì‘í•˜ì„¸ìš”!

## ëª…ë ¹ì–´:
- `/models` - ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ë³´ê¸°
- `/model <ì´ë¦„>` - ëª¨ë¸ ë³€ê²½
- `/clear` - ëŒ€í™” ë‚´ì—­ ì´ˆê¸°í™”
- `/system <ë©”ì‹œì§€>` - ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ì„¤ì •
- `quit` - ì¢…ë£Œ"""),
            title="í™˜ì˜í•©ë‹ˆë‹¤",
            border_style="blue"
        )
        chat_log.write(welcome_panel)

    def check_ollama_connection(self) -> bool:
        """Ollama ì„œë²„ ì—°ê²° í™•ì¸"""
        try:
            response = requests.get(f"{self.ollama_url}/api/tags", timeout=3)
            if response.status_code == 200:
                self.print_system_message("âœ… Ollama ì„œë²„ ì—°ê²°ë¨")
                return True
            else:
                self.print_system_message("âŒ Ollama ì„œë²„ ì‘ë‹µ ì˜¤ë¥˜")
                return False
        except requests.exceptions.RequestException:
            self.print_system_message("âŒ Ollama ì„œë²„ì— ì—°ê²°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\n\n**í•´ê²° ë°©ë²•:**\n1. `ollama serve` ëª…ë ¹ì–´ë¡œ ì„œë²„ ì‹œì‘\n2. `ollama pull qwen3:32b` ëª…ë ¹ì–´ë¡œ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ")
            return False
    
    def initialize_chat_model(self):
        """ChatOllama ëª¨ë¸ ì´ˆê¸°í™”"""
        try:
            self.chat_model = ChatOllama(
                model=self.model_name,
                base_url=self.ollama_url,
                temperature=0.7,
                top_p=0.9,
                streaming=True,  # ìŠ¤íŠ¸ë¦¬ë° í™œì„±í™”
                # num_predict=500,  # max_tokens ëŒ€ì‹  ì‚¬ìš©
            )
            # ê¸°ë³¸ ì‹œìŠ¤í…œ ë©”ì‹œì§€ ì„¤ì •
            self.messages = [
                SystemMessage(content="ë‹¹ì‹ ì€ ë„ì›€ì´ ë˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. í•œêµ­ì–´ë¡œ ì¹œê·¼í•˜ê³  ìì„¸í•˜ê²Œ ë‹µë³€í•´ì£¼ì„¸ìš”.")
            ]
            self.print_system_message(f"ğŸ¤– ChatOllama ëª¨ë¸ ì´ˆê¸°í™” ì™„ë£Œ: **{self.model_name}**")
        except Exception as e:
            self.print_system_message(f"âŒ ëª¨ë¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {str(e)}")
    
    def load_available_models(self):
        """ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ëª©ë¡ ë¡œë“œ"""
        try:
            response = requests.get(f"{self.ollama_url}/api/tags", timeout=3)
            if response.status_code == 200:
                data = response.json()
                self.available_models = [model['name'] for model in data.get('models', [])]
                if not self.available_models:
                    self.print_system_message("âš ï¸ ì„¤ì¹˜ëœ ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤.\n\n`ollama pull qwen3:32b`ë¥¼ ì‹¤í–‰í•´ì£¼ì„¸ìš”.")
            else:
                self.print_system_message("âš ï¸ ëª¨ë¸ ëª©ë¡ì„ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        except Exception as e:
            self.print_system_message(f"âš ï¸ ëª¨ë¸ ëª©ë¡ ë¡œë“œ ì˜¤ë¥˜: {str(e)}")

    def print_system_message(self, message: str):
        """ì‹œìŠ¤í…œ ë©”ì‹œì§€ë¥¼ RichLogì— ì¶œë ¥"""
        chat_log = self.query_one("#chat-log")
        panel = Panel(
            RichMarkdown(message),
            title="ì‹œìŠ¤í…œ",
            border_style="yellow"
        )
        chat_log.write(panel)

    def print_user_message(self, message: str):
        """ì‚¬ìš©ì ë©”ì‹œì§€ë¥¼ RichLogì— ì¶œë ¥"""
        chat_log = self.query_one("#chat-log")
        panel = Panel(
            RichMarkdown(message),
            title="You",
            border_style="blue"
        )
        chat_log.write(panel)

    def print_ai_message(self, message: str):
        """AI ë©”ì‹œì§€ë¥¼ RichLogì— ì¶œë ¥"""
        chat_log = self.query_one("#chat-log")
        panel = Panel(
            RichMarkdown(message),
            title="AI",
            # border_style="green"
        )
        chat_log.write(panel)
    
    def update_ai_response(self, partial_message: str):
        """ì‹¤ì‹œê°„ìœ¼ë¡œ AI ì‘ë‹µì„ ì—…ë°ì´íŠ¸"""
        streaming_widget = self.query_one("#streaming-response")
        
        # ìŠ¤íŠ¸ë¦¬ë° ìƒíƒœê°€ ì•„ë‹ˆë¼ë©´ ìœ„ì ¯ì„ í‘œì‹œí•˜ê³  ì‹œì‘
        if not hasattr(self, '_streaming_active') or not self._streaming_active:
            self._streaming_active = True
            streaming_widget.remove_class("hidden")
        
        # ìŠ¤íŠ¸ë¦¬ë° ìœ„ì ¯ ë‚´ìš© ì—…ë°ì´íŠ¸
        # panel = Panel(
        #     RichMarkdown(partial_message + " â–Œ"),
        #     title="AI (ì‘ì„± ì¤‘...)",
        #     border_style="yellow"
        # )
        streaming_widget.update(RichMarkdown(partial_message + " â–Œ"))
    
    def finalize_ai_response(self, final_message: str):
        """ìŠ¤íŠ¸ë¦¬ë° ì™„ë£Œ í›„ ìµœì¢… AI ì‘ë‹µ ì¶œë ¥"""
        chat_log = self.query_one("#chat-log")
        streaming_widget = self.query_one("#streaming-response")
        
        # ìµœì¢… ì‘ë‹µì„ ì±„íŒ… ë¡œê·¸ì— ì¶”ê°€
        # panel = Panel(
        #     RichMarkdown(final_message),
        #     title="AI",
        #     # border_style="green"
        # )
        chat_log.write(RichMarkdown(final_message))
        
        # ìŠ¤íŠ¸ë¦¬ë° ìœ„ì ¯ ìˆ¨ê¸°ê¸° ë° ìƒíƒœ ì¢…ë£Œ
        streaming_widget.add_class("hidden")
        streaming_widget.update("")  # ë‚´ìš© ì´ˆê¸°í™”
        self._streaming_active = False

    def on_key(self, event: Key) -> None:
        """í‚¤ ì…ë ¥ ì²˜ë¦¬"""
        # í¬ì»¤ìŠ¤ëœ ìœ„ì ¯ì´ ì…ë ¥ì°½ì¸ì§€ í™•ì¸
        focused = self.focused
        if focused and focused.id == "chat-input":
            input_widget = focused
            current_value = input_widget.value
            
            # ë°±ìŠ¤í˜ì´ìŠ¤ í‚¤ì´ê³  í˜„ì¬ ê°’ì´ "> " ì´í•˜ì¸ ê²½ìš° ì´ë²¤íŠ¸ ì°¨ë‹¨
            if event.key == "backspace" and len(current_value) <= 2:
                if current_value.startswith(">"):
                    event.prevent_default()
                    input_widget.value = "> "
                    # ì»¤ì„œë¥¼ ëìœ¼ë¡œ ì´ë™
                    input_widget.cursor_position = len(input_widget.value)
            
            # ë¶™ì—¬ë„£ê¸° ë‹¨ì¶•í‚¤ ì²˜ë¦¬ (Cmd+V ë˜ëŠ” Ctrl+V)
            elif event.key == "ctrl+v" or event.key == "cmd+v":
                # Textualì˜ ê¸°ë³¸ ë¶™ì—¬ë„£ê¸° ë™ì‘ì„ í—ˆìš©
                pass
            
            # ë³µì‚¬ ë‹¨ì¶•í‚¤ ì²˜ë¦¬ (Cmd+C ë˜ëŠ” Ctrl+C)
            elif event.key == "ctrl+c" or event.key == "cmd+c":
                # ì…ë ¥ì°½ì—ì„œì˜ ë³µì‚¬ëŠ” Textualì˜ ê¸°ë³¸ ë™ì‘ í—ˆìš©
                pass
        
        # ì „ì—­ ë³µì‚¬ ë‹¨ì¶•í‚¤ ì²˜ë¦¬ (í™”ë©´ì˜ ëª¨ë“  ì„ íƒëœ í…ìŠ¤íŠ¸)
        elif event.key == "ctrl+c" or event.key == "cmd+c":
            try:
                # í˜„ì¬ ì„ íƒëœ í…ìŠ¤íŠ¸ê°€ ìˆëŠ”ì§€ í™•ì¸
                selected_text = self.get_selected_text()
                if selected_text:
                    pyperclip.copy(selected_text)
                    event.prevent_default()
            except:
                # pyperclipì´ ì—†ê±°ë‚˜ ì˜¤ë¥˜ ë°œìƒ ì‹œ ê¸°ë³¸ ë™ì‘ í—ˆìš©
                pass

    def get_selected_text(self) -> str:
        """í˜„ì¬ ì„ íƒëœ í…ìŠ¤íŠ¸ë¥¼ ê°€ì ¸ì˜¤ê¸°"""
        try:
            # Textualì˜ í˜„ì¬ ì„ íƒ ì˜ì—­ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ
            # ì´ëŠ” Textualì˜ ë‚´ë¶€ êµ¬ì¡°ì— ë”°ë¼ ë‹¬ë¼ì§ˆ ìˆ˜ ìˆìŒ
            if hasattr(self, '_selected_text'):
                return self._selected_text
            return ""
        except:
            return ""

    def on_paste(self, _event: Paste) -> None:
        """ë¶™ì—¬ë„£ê¸° ì´ë²¤íŠ¸ ì²˜ë¦¬"""
        # í¬ì»¤ìŠ¤ëœ ìœ„ì ¯ì´ ì…ë ¥ì°½ì¸ì§€ í™•ì¸
        focused = self.focused
        if focused and focused.id == "chat-input":
            input_widget = focused
            
            # í˜„ì¬ ì»¤ì„œ ìœ„ì¹˜ í™•ì¸
            cursor_pos = input_widget.cursor_position
            
            # í”„ë¡¬í”„íŠ¸ ë¶€ë¶„("> ")ì„ ë³´í˜¸í•˜ë©´ì„œ ë¶™ì—¬ë„£ê¸°
            if cursor_pos < 2:
                # í”„ë¡¬í”„íŠ¸ ì•ì— ë¶™ì—¬ë„£ê¸° ì‹œë„ ì‹œ í”„ë¡¬í”„íŠ¸ ë’¤ë¡œ ì´ë™
                cursor_pos = 2
                input_widget.cursor_position = cursor_pos
            
            # ê¸°ë³¸ ë¶™ì—¬ë„£ê¸° ë™ì‘ í—ˆìš©
            # Textualì´ ìë™ìœ¼ë¡œ ì²˜ë¦¬í•˜ë„ë¡ í•¨


    @on(Input.Submitted)
    async def on_input_submitted(self, event: Input.Submitted) -> None:
        """ì‚¬ìš©ì ì…ë ¥ ì²˜ë¦¬"""
        raw_input = event.value.strip()
        
        # '> ' í”„ë¡¬í”„íŠ¸ ì œê±°
        if raw_input.startswith("> "):
            user_input = raw_input[2:].strip()
        else:
            user_input = raw_input
        
        if not user_input:
            # ì…ë ¥ì´ ì—†ìœ¼ë©´ í”„ë¡¬í”„íŠ¸ë§Œ ë‹¤ì‹œ ì„¤ì •
            event.input.value = "> "
            return
            
        # ì…ë ¥ì°½ì„ í”„ë¡¬í”„íŠ¸ë¡œ ì´ˆê¸°í™”
        event.input.value = "> "
        
        # ì¢…ë£Œ ëª…ë ¹ì–´ ì²˜ë¦¬
        if user_input.lower() in ['quit', 'exit', 'ì¢…ë£Œ']:
            self.exit()
            return
        
        # ëª…ë ¹ì–´ ì²˜ë¦¬
        if user_input.startswith('/'):
            await self.handle_command(user_input)
            return
            
        # ëª¨ë¸ì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ë‹¤ë©´ ì²˜ë¦¬ ì¤‘ë‹¨
        if not self.chat_model:
            await self.show_error_message("âŒ ëª¨ë¸ì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. Ollama ì„œë²„ë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")
            return
            
        # ì‚¬ìš©ì ë©”ì‹œì§€ë¥¼ Richë¡œ ì¶œë ¥
        self.print_user_message(user_input)
        
        # ë©”ì‹œì§€ íˆìŠ¤í† ë¦¬ì— ì¶”ê°€
        self.messages.append(HumanMessage(content=user_input))
        
        # AI ì‘ë‹µ ìƒì„± ì‹œì‘
        self.generate_response()

    async def handle_command(self, command: str):
        """ëª…ë ¹ì–´ ì²˜ë¦¬"""
        
        if command == "/models":
            if self.available_models:
                models_text = "**ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ë“¤:**\n\n" + "\n".join(f"â€¢ {model}" for model in self.available_models)
                models_text += f"\n\n**í˜„ì¬ ëª¨ë¸:** {self.model_name}"
                self.print_system_message(models_text)
            else:
                self.print_system_message("âŒ ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤.\n\n`ollama pull <ëª¨ë¸ëª…>`ìœ¼ë¡œ ëª¨ë¸ì„ ë‹¤ìš´ë¡œë“œí•˜ì„¸ìš”.")
                
        elif command.startswith("/model "):
            new_model = command.split(" ", 1)[1].strip()
            if new_model in self.available_models:
                self.model_name = new_model
                self.initialize_chat_model()  # ìƒˆ ëª¨ë¸ë¡œ ì¬ì´ˆê¸°í™”
                self.print_system_message(f"âœ… ëª¨ë¸ì´ **{new_model}**ë¡œ ë³€ê²½ë˜ì—ˆìŠµë‹ˆë‹¤.")
            else:
                self.print_system_message(f"âŒ ëª¨ë¸ '{new_model}'ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\n\n`/models`ë¡œ ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ì„ í™•ì¸í•˜ì„¸ìš”.")
                
        elif command.startswith("/system "):
            system_prompt = command.split(" ", 1)[1].strip()
            # ê¸°ì¡´ ì‹œìŠ¤í…œ ë©”ì‹œì§€ êµì²´
            self.messages = [SystemMessage(content=system_prompt)] + [msg for msg in self.messages if not isinstance(msg, SystemMessage)]
            self.print_system_message(f"âœ… ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ê°€ ì„¤ì •ë˜ì—ˆìŠµë‹ˆë‹¤:\n\n*{system_prompt}*")
                
        elif command == "/clear":
            # ì‹œìŠ¤í…œ ë©”ì‹œì§€ë§Œ ìœ ì§€í•˜ê³  ë‚˜ë¨¸ì§€ ì´ˆê¸°í™”
            system_messages = [msg for msg in self.messages if isinstance(msg, SystemMessage)]
            self.messages = system_messages if system_messages else [
                SystemMessage(content="ë‹¹ì‹ ì€ ë„ì›€ì´ ë˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. í•œêµ­ì–´ë¡œ ì¹œê·¼í•˜ê³  ìì„¸í•˜ê²Œ ë‹µë³€í•´ì£¼ì„¸ìš”.")
            ]
            self.print_system_message("âœ… ëŒ€í™” ë‚´ì—­ì´ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤.")
            
        else:
            self.print_system_message(f"âŒ ì•Œ ìˆ˜ ì—†ëŠ” ëª…ë ¹ì–´ì…ë‹ˆë‹¤: {command}\n\nì‚¬ìš© ê°€ëŠ¥í•œ ëª…ë ¹ì–´ë¥¼ ë³´ë ¤ë©´ ì²˜ìŒ ë©”ì‹œì§€ë¥¼ ì°¸ê³ í•˜ì„¸ìš”.")

    async def show_error_message(self, message: str):
        """ì˜¤ë¥˜ ë©”ì‹œì§€ í‘œì‹œ"""
        self.print_system_message(message)

    @work(thread=True)
    def generate_response(self) -> None:
        """AI ì‘ë‹µ ìƒì„± (LangChain ìŠ¤íŠ¸ë¦¬ë° ì‚¬ìš©)"""
        try:
            # ìµœê·¼ 10ê°œ ë©”ì‹œì§€ë§Œ ìœ ì§€ (ë©”ëª¨ë¦¬ ì ˆì•½)
            if len(self.messages) > 11:  # ì‹œìŠ¤í…œ ë©”ì‹œì§€ + 10ê°œ
                system_msgs = [msg for msg in self.messages if isinstance(msg, SystemMessage)]
                recent_msgs = self.messages[-10:]
                self.messages = system_msgs + recent_msgs
            
            response_content = ""
            
            # ìŠ¤íŠ¸ë¦¬ë° ìƒíƒœ ì´ˆê¸°í™”
            self._streaming_active = False
            
            # LangChain ìŠ¤íŠ¸ë¦¬ë° í˜¸ì¶œ ì‚¬ìš©
            for chunk in self.chat_model.stream(self.messages):
                if hasattr(chunk, 'content') and chunk.content:
                    response_content += chunk.content
                    # ì‹¤ì‹œê°„ìœ¼ë¡œ AI ì‘ë‹µ ì—…ë°ì´íŠ¸
                    self.call_from_thread(self.update_ai_response, response_content)
            
            # ìŠ¤íŠ¸ë¦¬ë° ì™„ë£Œ í›„ ìµœì¢… ë©”ì‹œì§€ ì¶œë ¥
            if response_content:
                self.messages.append(AIMessage(content=response_content))
                # ìŠ¤íŠ¸ë¦¬ë° ì™„ë£Œ í‘œì‹œ
                self._streaming_active = False
                self.call_from_thread(self.finalize_ai_response, response_content)
            
        except Exception as e:
            error_msg = f"âŒ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}\n\nëª¨ë¸ì´ ë‹¤ìš´ë¡œë“œë˜ì–´ ìˆëŠ”ì§€ í™•ì¸í•´ì£¼ì„¸ìš”."
            self.call_from_thread(self.print_system_message, error_msg)

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    print("ğŸ¦™ LangChain Ollama ì±„íŒ… ì•±ì„ ì‹œì‘í•©ë‹ˆë‹¤...")
    print("\nğŸ“‹ ì‚¬ì „ ì¤€ë¹„ì‚¬í•­:")
    print("1. ollama serve")
    print("2. ollama pull qwen3:32b")
    print("3. python app.py\n")
    
    app = LangChainOllamaChat()
    app.run()

if __name__ == "__main__":
    main()
