import asyncio
from rich import print
from rich.console import Console
from langchain_ollama import ChatOllama
from langchain_mcp_adapters.client import MultiServerMCPClient
from langgraph.prebuilt import create_react_agent

async def simple_test():
    """ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸"""
    model = ChatOllama(model="qwen3:30b-32k")
    
    servers = {
        "sequential-thinking": {
            "command": "npx", 
            "args": ["-y", "@modelcontextprotocol/server-sequential-thinking"],
            "transport": "stdio",
            "env": {
                "DISABLE_THOUGHT_LOGGING": "true"
            }
        }
    }
    
    client = MultiServerMCPClient(servers)
    tools = await client.get_tools()
    agent = create_react_agent(model, tools)
    
    console = Console()
    with console.status("[green]ìˆ˜í•™ ë¬¸ì œ í’€ì´ ì¤‘...\n", spinner="dots"):
        response = await agent.ainvoke({
            "messages": "ê°„ë‹¨í•œ ìˆ˜í•™ ë¬¸ì œë¥¼ ë‹¨ê³„ë³„ë¡œ í’€ì–´ì£¼ì„¸ìš”: 2x + 3 = 11"
        })
    
    print("ğŸ” Sequential Thinking ê²°ê³¼:")
    print(response)

if __name__ == "__main__":
    # ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    asyncio.run(simple_test())
