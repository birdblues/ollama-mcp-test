import asyncio
from rich import print
from langchain_ollama import ChatOllama
from langchain_mcp_adapters.client import MultiServerMCPClient
from langgraph.prebuilt import create_react_agent


async def simple_test():
    """ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸"""
    model = ChatOllama(model="qwen3:32b")
    
    servers = {
        "sequential-thinking": {
            "command": "npx", 
            "args": ["-y", "@modelcontextprotocol/server-sequential-thinking"],
            "transport": "stdio"
        }
    }
    
    client = MultiServerMCPClient(servers)
    tools = await client.get_tools()
    agent = create_react_agent(model, tools)
    
    response = await agent.ainvoke({
        "messages": "ê°„ë‹¨í•œ ìˆ˜í•™ ë¬¸ì œë¥¼ ë‹¨ê³„ë³„ë¡œ í’€ì–´ì£¼ì„¸ìš”: 2x + 3 = 11"
    })
    
    print("ğŸ” Sequential Thinking ê²°ê³¼:")
    if hasattr(response, 'messages') and response.messages:
        print(response.messages[-1].content)
    else:
        print(response)

if __name__ == "__main__":
    print("ğŸš€ Sequential Thinking MCP ì„œë²„ í…ŒìŠ¤íŠ¸ ì‹œì‘...")
    
    # ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    print("\n1ï¸âƒ£ ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸:")
    asyncio.run(simple_test())